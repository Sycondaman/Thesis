\chapter{Introduction}

The advent of general purpose programming on the graphics programming unit (GPU), commonly referred to as GPGPU, has led to a dramatic shift in the way computers are used to solve problems. In June 2010 the organization Top 500, which maintains a ranking of the top 500 fastest supercomputers, ranked a GPU based system in the \#2 spot, the first time any GPU based system had reached the top 10 \cite{top500:2010}. In June 2011 three GPU based systems were in the top 10, and in November 2012 the supercomputer Titan took the \#1 spot using 18,688 nVidia GPUs \cite{top500:2011} \cite{top500:2012}.

The high availability of discrete GPUs and the ease of development provided by GPGPU APIs have helped provide access to massively parallel programming at all levels, not just in the realm of supercomputing. Consumer applications such as Adobe Premiere \cite{adobe} and Blender \cite{blender} are also taking advantage of GPGPU to improve performance. A software development kit called PhysX is used by game developers to move physics calculations onto the GPU for more realistic rendering \cite{physx}.

The key benefit of GPGPU is the switch from serial to massively parallel processing, allowing algorithms working with large and complex data sets to operate much more efficiently. Artificial intelligence (AI) algorithms stand to benefit from this paradigm shift, allowing for more complex algorithms and better performance in existing algorithms. This, in turn, benefit AI applications such as large data analysis problems, virtual assistants, and natural language processors. However, to take advantage of these benefits algorithms must be designed with the compute constraints imposed by the GPU architecture in mind.

Reinforcement learning is a class of machine learning algorithms that works by rewarding good behavior, reinforcing policies that lead to the maximum reward. Reinforcement learning algorithms are very well studied, are applicable to a wide variety of problems, and have a low barrier to entry for developers. With this in mind, we choose to focus our efforts on the benefits of massively parallel reinforcement learning. To demonstrate the benefits we need an environment which places strict performance constraints on the AI algorithm in addition to functional design requirements. For this purpose we choose the video game Quake III.

Quake III is a first person shooter in which the computer controls agents which oppose the human player. The computer agents are expected to compete with human players ranging from novices to experts. In addition, the agents also must have unique, human-like personalities. In addition to design constraints, AI agents in Quake III are heavily performance constrained. A common number given for the amount of time available for making a single decision is $1-3$ ms \cite{game:ai:lecture}. This number comes from the assumption that a single frame of video for the game takes $33$ ms and the AI accounts for around ten percent of this calculation. In reality, many AI calculations are done across multiple frames to get around this performance constraint. Even if we amortize over the course of one second, all of the game's AI agents have around $90$ ms to make decisions.

We demonstrate the use of hardware acceleration and iterative update off-line reinforcement learning as a solution to the performance constraints in video game AI. Hardware acceleration allows certain algorithms to perform faster than traditional computation by an order of magnitude or more. We believe combining this speed increase with a policy which is periodically updated by a background process will allow more realistic automated agents to function in a wide variety of performance constrained environments.