\chapter{Introduction}

The earliest video games were designed to pit multiple players against one another, which avoided the need for any computer controlled agents. It was not long, however, before games were released which pit a human player against a computer agent in simple simulations, such as Pong. These early artificial intelligence (AI) agents were programmed to follow a very simple and strict set of rules which allowed them to compete with humans. As computer hardware advanced, so to did the complexity of video game environments. This increase in the environment complexity required increasingly clever AI techniques to maintain the illusion that the computer controlled agents possessed human-like intelligence. \cite{game:ai:history}

Despite the increasing complexity and realism, AI in video games has always lagged behind AI in academia. The video game F.E.A.R. which was released in 2005 was applauded for excellent AI agents. A paper was presented in 2006 detailing the implementation of F.E.A.R.'s AI system, which was based on the STRIPS planning technique developed in 1970. \cite{fear} There are a number of possible reasons to explain this delay in AI technique adoption; however, the two most clear reasons are the performance constraints and design considerations. \cite{game:ai:history}

The purpose of a video game is most commonly to entertain, although they can be used for education as well. Because of this, video game AI is intended to both present the illusion of a intelligent computer agents and to ensure the human player(s) is having fun. In academia AI is most commonly studied to provide superior performance on tasks, which does not always equate to more enjoyment when used in opposition to a human player.

In addition to design constraints, AI agents in modern games are often heavily performance constrained. A common number given for the performance constraints is $1-3$ ms. \cite{game:ai:lecture} This number comes from the assumption that a single frame of video for the game takes $33$ ms and the AI accounts for around ten percent of this calculation. In reality, many AI calculations are done across multiple frames to get around this performance constraint. However, even if you amortize over the course of one second, all of the game's AI agents have around $90$ ms to make decisions.

Reinforcement learning techniques provide effective and intuitive solutions to the design problem: to control the behavior of the agent simply change the reward scheme such that the agent maximizes reward by challenging human players while still allowing them to win. The performance problem is a more challenging problem and one which video games typically solve through clever scripts or AI cheating. \cite{game:ai:history}

In this paper, we propose the use of hardware acceleration and iterative update off-line reinforcement learning as a solution to the performance constraints in video game AI. Hardware acceleration allows certain algorithms to perform faster than traditional computation by an order of magnitude or more. Combining this speed increase with a policy which is periodically updated by a background process allows the agent to make decisions very quickly while still adapting to its environment and maintaining optimal behavior.