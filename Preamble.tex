\clearpage\pagenumbering{roman}

\title{Massively Parallel Reinforcement Learning With an Application to Video Games}
\author{TYLER GOERINGER}
\date{August 2013}
\advisorname{Soumya Ray}
\department{Electrical Engineering and Computer Science}
\maketitle


\committeechair{Soumya Ray}
\committeeb{Michael Lewicki}
\committeea{Swarup Bhunia}
\committeec{Frank Merat}
\committeeapprovalpage

\begin{acknowledgments}
First and foremost I would like to thank my adviser Dr. Soumya Ray. When I approached him with my research topic he was enthusiastic and supportive. When I encountered problems he provided advice and guidance which has made this research possible. From the start to the finish, this research would not have been possible without him.
    
I would like to thank my boss, Chris Holmes, for supporting me and approving the time off I needed to work on research. In addition I give my thanks to all those who provided technical assistance: Simon Layton, for helping me understand the CUDA API and how to best utilize it; Thomas Minor and Tyler Mecke, my coworkers who answered an endless stream of C\textbackslash C++ questions; and Hitesh Shah, my coworker who provided advice on integrating into the Quake III engine.

Last but not least, I would like to express my gratitude to my parents and my sister. They have supported me in my endeavors for as long as I can remember, even when I did not acknowledge it. Without their support I would not have made it to Case and started the path that has lead me here.
\end{acknowledgments}

\tableofcontents
\listoftables
\listoffigures

\begin{abbreviations}
    \textbf{AAS:} Area Awareness System

    \textbf{AI:} Artificial Intelligence

    \textbf{API:} Application Programming Interface
    
    \textbf{BLAS:} Basic Linear Algebra Subprograms
    
    \textbf{CPU:} Central Processing Unit
    
    \textbf{FPS:} First Person Shooter
    
    \textbf{GPGPU:} General Purpose Programming on the GPU
    
    \textbf{GPU:} Graphics Processing Unit
    
    \textbf{HTN:} Hierarchical Task Network
    
    \textbf{LSPI:} Least-Squares Policy Iteration
    
    \textbf{MDP:} Markov Decision Process
    
    \textbf{SIMD:} Single Instruction, Multiple Data
\end{abbreviations}

\begin{umiabstract}
    We propose a framework for periodic policy updates of computer controlled agents in an interactive scenario. We use the graphics processing unit (GPU) to accelerate an offline reinforcement learning algorithm which periodically updates an online agent's policy. The main contributions of this work are the use of GPU acceleration combined with a periodic update model to provide reinforcement learning in a performance constrained environment. We show empirically that given certain environment properties, the GPU accelerated implementation provides better performance than a traditional implementation utilizing the central processing unit (CPU). In addition, we show that while an online machine learning algorithm can function in some performance constrained environments, an offline algorithm reduces the performance constraints allowing for application to a wider variety of environments. Finally, we demonstrate combining these techniques to control an agent in the world of Quake III Arena, resulting in a computer controlled agent capable of adapting to different opponents.
\end{umiabstract}

\clearpage\pagenumbering{arabic}