\chapter{Conclusion and Future Work}
\label{chap:conclusion}

We have proposed a framework for reinforcement learning using GPGPU in performance constrained environments. We described the use of LSPI in conjunction with GPU acceleration and iterative updates to create a semi-onlin learning agent. We demonstrated empirically that this model can reduce the performance requirements for an agent while interacting with an environment. We also demonstrated that in some environments the GPU can provide better performance for LSPI than traditional CPU implementations.

Although we demonstrated improvements using the GPU, environments with compact environment representations may perform better with CPU implementations. It may be possible to better utilize the GPU by unrolling the main loop in LSPI, maximizing the input size of operations which in turn maximizes throughput. Additionally, our online model for LSPI requires all samples collected to be processed at each iteration, resulting in increasingly infrequent policy updates. It would potentially be more beneficial to apply our model to an offline reinforcement learning algorithm which weights the current policy against the samples being processed. This would eliminate the need to analyze the same samples each time the policy needs to be updated.

Finally, the environment in which we demonstrated our agent is old enough that it is not performance constrained on modern hardware. It would be interesting to see this technique applied in an environment with tighter performance constraints to determine if the benefit is sufficient to warrant its use.