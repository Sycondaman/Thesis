\chapter{Massively Parallel Learning Algorithms}

As discussed in the background section, offline learning algorithms have the potential to provide machine learning behavior for agents acting in performance constrained environments. To demonstrate this potential we chose to us Least-Squares Policy Iteration (LSPI) and compare it to an online policy gradient algorithm. This chapter presents these algorithms in mathematical detail and suggests a modification to LSPI to support its use as an online algorithm for performance constrained environments. Some of the derivation steps of the core algorithms have been left out, for more information please see the referenced sources. Details on how we integrated these algorithms in the Quake III environment can be found in the `Experiments' chapter.

\section{Least-Squares Policy Iteration}

Least-Squares Policy Iteration (LSPI) is an offline, model-free reinforcement learning algorithm proposed by Michail Lagoudakis and Ronald Parr. \cite{lspi} LSPI takes as input a set of samples from the environment and performs approximate policy iteration using a function called LTSD$Q$ to compute the state-action value function during the policy evaluation step. LSTD$Q$ stands for Least-Squares Temporal-Difference Learning for the State-Action Value Function; in other words, it is a least-squares temporal difference algorithm for learning $\hat{Q}^\pi$, the least-squares fixed-point approximation to the sate-action value function, $Q^\pi$.

LSPI uses a linear architecture for $\hat{Q}^\pi$, which means the values are approximated using a linear combination of $k$ basis functions, commonly referred to as features:

\[
    \hat{Q}^\pi(s,a;w) = \sum_{j=1}^k \phi_j(s,a)w_j
\]

In this formula, $\phi(s,a)$ is a fixed function of $s$ and $a$ and $w_j$ represents the weight for $\phi_j$. We can represent $\phi(s,a)$ as a column vector with an entry for each basis function:


\[
    \phi(s,a) = \begin{pmatrix}
       \phi_1(s,a) \\ \phi_2(s,a) \\ \vdots \\ \phi_k(s,a)
    \end{pmatrix}
\]

$\hat{Q}^\pi$  can now be expressed compactly:

\[
    \hat{Q}^\pi = \Phi w^\pi
\]
where $w^\pi$ is a column vector with length k and $\Phi$ is a matrix of the form

\[
    \Phi = \begin{pmatrix}
        \phi(s_1,a_1,)^T \\ \vdots \\ \phi(s,a)^T \\ \vdots \\ \phi(s_{|\mathcal{S}|},a_{|\mathcal{A}|})^T
    \end{pmatrix}
\]
Each row of $\Phi$ represents the basis function values for a specific state-action pair. The basis functions are required to be linearly independent in LSPI which also means the columns of $\Phi$ are linearly independent.

To find $\hat{Q}^\pi$, we force it to be a fixed point under the Bellman operator:

\[
    T_\pi\hat{Q}^\pi \approx \hat{Q}^\pi
\]
To ensure this is true we must project $T_\pi\hat{Q}^\pi$ to a point in the space spanned by the basis functions:

\[
    \hat{Q}^\pi = \Phi(\Phi^T\Phi)^{-1}\Phi^T(\mathcal{R} + \gamma P\Pi_\pi\hat{Q}^\pi)
\]
From this formula we can derive the desired solution:
\[
    \Phi^T(\Phi - \gamma P\Pi_\pi\Phi)w^\pi = \Phi^T\mathcal{R}
\]

To achieve the final project, we define $\mu$ as a probability distribution over $(s,a)$ and $\Delta_\mu$ as a diagonal matrix with the projection weights $\mu(s,a)$. This results in the weighted least-squares fixed-point solution:

\[
    w^\pi = (\Phi^T\Delta_\mu(\Phi - \gamma P\Pi_\pi\Phi))^{-1}\Phi^T\Delta_\mu\mathcal{R}
\]

Once again assuming there are $k$ linearly independent basis functions, learning $\hat{Q}^\pi$ is equivalent to learning the weights $w^\pi$ of $\hat{Q}^\pi = \Phi w^\pi$. The exact values can be computed by solving the linear system

\[
    Aw^\pi = b
\]
where

\[
    \begin{array}{rcl}
        A &=& \Phi^T\Delta_\mu(\Phi - \gamma P\Pi_\pi\Phi) \\
        b &=& \Phi^T\Delta_\mu\mathcal{R}
    \end{array}
\]
and $\mu$ is a probability distribution over the state-action space which defines the weights of the projection. $A$ and $b$ can be learned using samples and the learned linear system can be solved for $\tilde{w}^\pi$. 

The equations for $A$ and $b$ can be reduced until it is clear that $A$ is the sum of many rank one matrices of the form:

\[
    \phi(s,a)(\phi(s,a) - \gamma\phi(s',\pi(s')))^T
\]
and $b$ is the sum of many vectors of the form:

\[
    \phi(s,a)R(s,a,s')
\]
These summations are then taken over $s$, $a$, and $s'$ and weighted according to $\mu(s,a)$ and $\mathcal{P}(s,a,s')$. It is generally impractical to compute this summation over all values of $(s,a,s')$; however, samples taken from the environment can be used to obtain arbitrarily close approximations.

Given a finite set of samples of the form

\[
    D = { (s_i,a_i,r_i,s_i') | i = 1,2,...,L}
\]
we can learn $A$ and $b$:

\[
    \begin{array}{rcl}
        \tilde{A} &=& \frac{1}{L}\displaystyle\sum_{i=1}^L[\phi(s_i,a_i)(\phi(s_i,a_i) - \gamma\phi(s_i',\pi(s_i')))^T] \\
        \tilde{b} &=& \frac{1}{L}\displaystyle\sum_{i=1}^L[\phi(s_i,a_i)r_i)]
    \end{array}
\]
In practical computations $L$ is finite, allowing us to drop the factor $(1/L)$ since it cancels out. Because samples contribute additively an incremental update rule can be constructed:

\[
    \begin{array}{rcl}
        \tilde{A}^{(t+1)} &=& \tilde{A}^{(t)} + \phi(s_t,a_t)(\phi(s_t,a_t) - \gamma\phi(s_t',\pi(s_t')))^T \\
        \tilde{b}^{(t+1)} &=& \tilde{b}^{(t)} + \phi(s_t,a_t)r_t
    \end{array}
\]

Using these formulas, an algorithm can be constrcuted to learn the weighted least-squares fixed-point approximation of $\hat{Q}^\pi$: LSTD$Q$. The name of this algorithm is derived due to similarities with LSTD, Least-Squares Temporal Difference Learning. With LSTD$Q$ the same set of samples $D$ can be used to learn $\hat{Q}^\pi$ of any policy as long as $\pi(s')$ exists for each $s' \in D$. Another nice feature of LSTD$Q$ is the lack of any restriction on how samples are collected. This means we can learn a policy from pre-existing samples, combine samples from multiple agents, or even take samples from user input.

In terms of performance, LSTD$Q$ requires $O(k^2)$ independent of the size of the state and action spaces. Each update to the matrices $\hat{A}$ and $\hat{b}$ has a cost of $O(k^2)$ and a one-time cost of $O(k^3)$ is required to solve the system. An optimized version of LSTD$Q$ is also possible using the Sherman-Morrison formula for a recursive least-squares computation of the inverse. Let $B^{(t)}$ be the inverse of $\hat{A}^{(t)}$:

\[
    B^{(t)} = B^{(t-1)} - \frac{B^{(t-1)}\phi(s_t,a_t)\big(\phi(s_t,a_t) - \gamma\phi(s_t',\pi(s_t'))\big)^TB^{(t-1)}}{1 + \phi(s_t,a_t) - \gamma\phi\big(s_t',\pi(s_t'))\big)^TB^{(t-1)}\phi(s_t,a_t)}
\]

\subsection{Online Least-Squares Policy Iteration}

\section{Policy Gradient}

The term `Policy Gradient' describes a class of machine learning techniques which apply gradient ascent optimization to the policy value function. Given the policy value function, $p(\theta)$, policy gradient algorithms can be succinctly described by the following formula:

\[
    \theta_{n+1} = \theta_{n} + \alpha\nabla_\theta p(\theta_{n})
\]
In this formula $\theta$ represents the policy parameters and $\alpha$ is referred to as the step size or learning rate. \cite{norvig}\cite{bishop} Despite the apparent simplicity of this formula, calculating the value of $\nabla_\theta p(\theta)$ is not always straightforward. In this section we provide the mathematical background for the particular online gradient policy algorithm we used for comparison purposes.

To calculate $\nabla_\theta p(\theta)$, we must first ensure that $p(\theta)$ is differentiable with respect to $\theta$. Assume we have a basis function, $J_\theta(s,a)$ which is used to generate a policy as follows:

\[
    \pi_\theta(s,a) = \argmax_a J_\theta(s,a)
\]
The use of the $argmax$ function may result in a discontinous distribution. This can be fixed very simply by using a probabilistic policy \cite{olpomdp:lecture}:

\[
    \pi_\theta(s,a) = Pr(a|s) = \frac{e^{J_\theta(s,a)/T}}{\sum\limits_{a'\in A} e^{J_\theta(s,a')/T}}
\]

Using this model we can then derive a value for $\nabla_\theta p(\theta)$:
\[
    \begin{aligned}
        \nabla_\theta p(\theta) &= \sum_a (\nabla_\theta\pi_\theta(s_0,a))R(a) \\
        &= \sum_a \pi_\theta(s_0, a) \nabla_\theta log(\pi_\theta(s_0, a))R(a) \\
    \end{aligned}
\]
After simplifying the gradient calculation we arrive at the following:
\[
    \begin{aligned}
        \nabla_\theta p(\theta) &= \sum_a\pi_\theta(s_0,a)g_\theta(s_0,a)R(a) \\
        g_\theta(s_0,a) &= f_\theta(s_0, a) - \sum\limits_{a'} \pi_\theta(s_0,a')f_\theta(s_0,a')
    \end{aligned}
\]

In order to estimate the gradient from samples we use basic Monte Carlo techniques: execute the policy while making small changes to $\theta$. The result is an online algorithm called OLPOMDP, proposed by Baxter, Bartlett, and Weaver. \cite{olpomdp} Since the policy involved is probablistic, exploration of the environment happens naturally when selecting actions. With each action we then update the eligibility trace, $e$, by estimating the value of the gradient for that action. The eligibility trace is then used to update $\theta$ according to the reward given for each sample in the environment. The value $\beta$ is referred to as the bias and controls the weight of previous experiences in the environment. Increasing $\beta$ biases the algorithm toward past experiences while also increasing the variance. The pseudocode for this algorithm is given in Algorithm \ref{olpomdp}.

\begin{algorithm}
\caption{OLPOMDP}
\label{olpomdp}
    \begin{algorithmic}[1]
        \REQUIRE
            \begin{itemize} 
                \item $\beta \in [0,1)$ 
                \item $T > 0$ 
                \item $\theta_0 \in \mathbb{R}^K$ 
            \end{itemize}
    \STATE Set $e = 0$
    \FOR{$t = 0$ to $\infty$}
        \STATE Observe the current state, $s$
        \STATE Select action $a$ according to $\pi_\theta(s)$
        \STATE Execute $a$
        \STATE Observe reward $r$
        \STATE $e \leftarrow \beta e + f(s,a) - \sum\limits_{a'}\pi_\theta(s,a')f_\theta(s,a')$
        \STATE $\theta \leftarrow + \alpha \cdot r \cdot e$
    \ENDFOR
    \end{algorithmic}
\end{algorithm}

