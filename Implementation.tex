\chapter{Massively Parallel Reinforcement Learning}
\label{chap:implementation}

As discussed in the background section, offline learning algorithms have the potential to learn behavior for agents acting in performance constrained environments. To demonstrate this potential we chose to use Least-Squares Policy Iteration (LSPI) and compare it to an online policy gradient algorithm. This chapter presents LSPI in mathematical detail and suggests a modification to LSPI to support its use as an online algorithm for performance constrained environments. Some of the derivation steps of LSPI have been left out, for more information please see the referenced sources. The last section in this chapter describes the integration of LSPI in the Quake III environment.

\section{Least-Squares Policy Iteration}
\label{chap:implementation:lspi}

Least-Squares Policy Iteration (LSPI) is an offline, model-free reinforcement learning algorithm proposed by Michail Lagoudakis and Ronald Parr \cite{lspi}. LSPI takes as input a set of samples from the environment and performs approximate policy iteration using a function called LTSD$Q$ to compute the state-action value function during the policy evaluation step.

The state-action value function $Q^\pi$ maps a state-action pair to a value reflecting its long-term utility. This means the optimal action $a$ for an agent in state $s$ is given by $\argmax\limits_a Q^\pi(s,a)$. LSTD$Q$ learns $\hat{Q}^\pi$, the least-squares fixed-point approximation to $Q^\pi$.

LSPI uses a linear architecture for $\hat{Q}^\pi$, which means the values are approximated using a linear combination of $k$ basis functions, commonly referred to as features:

\[
    \hat{Q}^\pi(s,a;w) = \sum_{j=1}^k \phi_j(s,a)w_j
\]

In this formula, $\phi(s,a)$ is a fixed function of $s$ and $a$ and $w_j$ represents the weight for $\phi_j$. We can represent $\phi(s,a)$ as a column vector with an entry for each basis function:

\[
    \phi(s,a) = \begin{pmatrix}
       \phi_1(s,a) \\ \phi_2(s,a) \\ \vdots \\ \phi_k(s,a)
    \end{pmatrix}
\]
$\hat{Q}^\pi$  can now be expressed compactly:

\[
    \hat{Q}^\pi = \Phi w^\pi
\]
where $w^\pi$ is a column vector with length $k$ and $\Phi$ is a matrix of the form

\[
    \Phi = \begin{pmatrix}
        \phi(s_1,a_1,)^T \\ \vdots \\ \phi(s,a)^T \\ \vdots \\ \phi(s_{|\mathcal{S}|},a_{|\mathcal{A}|})^T
    \end{pmatrix}
\]
Each row of $\Phi$ represents the basis function values for a specific state-action pair. The basis functions are required to be linearly independent in LSPI which also means the columns of $\Phi$ are linearly independent.

To find $\hat{Q}^\pi$, we force it to be a fixed point under $T_\pi$, the Bellman operator:
\[
    \hat{Q}^\pi \approx T_\pi\hat{Q}^\pi
\]
The Bellman operator is defined such that successive applications of $T_\pi$ to any initial vector $Q$ will converge on the state-action value function $Q^\pi$ \cite{lspi}. To ensure this is true we must project $T_\pi\hat{Q}^\pi$ to a point in the space spanned by the basis functions:

\[
    \hat{Q}^\pi = \Phi(\Phi^T\Phi)^{-1}\Phi^T(R + \gamma \mathcal{P}\Pi_\pi\hat{Q}^\pi)
\]
The matrix $\Pi_\pi$ describes policy $\pi$ such that $\Pi_\pi(s',(s',a')) = \phi(s',a')w^\pi$. From this formula we can derive the desired solution:
\[
    \Phi^T(\Phi - \gamma \mathcal{P}\Pi_\pi\Phi)w^\pi = \Phi^TR
\]

Assuming there are $k$ linearly independent basis functions, learning $\hat{Q}^\pi$ is equivalent to learning the weights $w^\pi$ of $\hat{Q}^\pi = \Phi w^\pi$. The exact values can be computed by solving the linear system

\begin{equation}
    Aw^\pi = b
\end{equation}
where

\[
    \begin{array}{rcl}
        A &=& \Phi^T(\Phi - \gamma P\Pi_\pi\Phi) \\
        b &=& \Phi^T\mathcal{R}
    \end{array}
\]

$A$ and $b$ can be learned using samples and the learned linear system can be solved for $\tilde{w}^\pi$. The equations for $A$ and $b$ can be reduced until it is clear that $A$ is the sum of many rank one matrices of the form:

\[
    \phi(s,a)(\phi(s,a) - \gamma\phi(s',\pi(s')))^T
\]
and $b$ is the sum of many vectors of the form:

\[
    \phi(s,a)R(s,a,s')
\]
These summations are then taken over $s$, $a$, and $s'$ and weighted according to $\mathcal{P}(s,a,s')$. The summations may be optionally weighted over a probability distribution $\mu(s,a)$ to correct for sampling bias. It is generally impractical to compute this summation over all values of $(s,a,s')$; however, samples taken from the environment can be used to obtain arbitrarily close approximations.

Given a finite set of samples of the form

\[
    D = { (s_i,a_i,r_i,s_i') | i = 1,2,...,L}
\]
we can learn $A$ and $b$:

\[
    \begin{array}{rcl}
        \tilde{A} &=& \displaystyle\sum_{i=1}^L[\phi(s_i,a_i)(\phi(s_i,a_i) - \gamma\phi(s_i',\pi(s_i')))^T] \\
        \tilde{b} &=& \displaystyle\sum_{i=1}^L[\phi(s_i,a_i)r_i)]
    \end{array}
\]
Because samples contribute additively an incremental update rule can be constructed:

\begin{equation}
\label{eq:lspi:update}
    \begin{array}{rcl}
        \tilde{A}^{(t+1)} &=& \tilde{A}^{(t)} + \phi(s_t,a_t)(\phi(s_t,a_t) - \gamma\phi(s_t',\pi(s_t')))^T \\
        \tilde{b}^{(t+1)} &=& \tilde{b}^{(t)} + \phi(s_t,a_t)r_t
    \end{array}
\end{equation}

Using Equation \ref{eq:lspi:update}, an algorithm can be constructed to learn the weighted least-squares fixed-point approximation of $\hat{Q}^\pi$: LSTD$Q$. A nice feature of LSTD$Q$ is the lack of any restriction on how samples are collected. This means we can learn a policy from pre-existing samples, combine samples from multiple agents, or even take samples from user input.

In terms of performance, LSTD$Q$ requires $O(k^2)$, where $k$ is the size of the basis. Note that it is independent of the size of the state-action space. Each update to the matrices $\hat{A}$ and $\hat{b}$ has a cost of $O(k^2)$ and a one-time cost of $O(k^3)$ is required to solve the system. We use an optimized version of LSTD$Q$ which uses the Sherman-Morrison formula for a recursive least-squares computation of the inverse \cite{lspi}. Here we compute $B^{(t)}$ as the inverse of $\hat{A}^{(t)}$:

\begin{equation}
\label{eq:lspi}
    B^{(t)} = B^{(t-1)} - \frac{B^{(t-1)}\phi(s_t,a_t)\big(\phi(s_t,a_t) - \gamma\phi(s_t',\pi(s_t'))\big)^TB^{(t-1)}}{1 + \big(\phi(s_t,a_t) - \gamma\phi(s_t',\pi(s_t'))\big)^TB^{(t-1)}\phi(s_t,a_t)}
\end{equation}
The pseudocode for LSPI is shown in Algorithm \ref{alg:lspi} and the pseudocode for LSTD$Q$ is shown in Algorithm \ref{lstdq} \cite{lspi}.

\begin{algorithm}
\caption{LSPI}
\label{alg:lspi}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \REQUIRE
            \begin{itemize} 
                \item $D$ - Samples of the form $(s,a,r,s')$ 
                \item $k$ - Number of basis functions
                \item $\phi$ - Basis functions
                \item $\gamma$ - Discount factor
                \item $\epsilon$ - Stopping criterion
                \item $w_0$ - Initial policy
            \end{itemize}
        \STATE $w' \leftarrow w_0$
        \REPEAT
            \STATE $w \leftarrow w'$
            \STATE $w' \leftarrow$ LSTD$Q (D, k, \phi, \gamma, w)$
        \UNTIL{$(\parallel w - w' \parallel < \epsilon)$}
        \RETURN $w$
    \end{algorithmic}
    }
\end{algorithm}

\begin{algorithm}
\caption{LSTD$Q$}
\label{lstdq}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \REQUIRE
            \begin{itemize} 
                \item $D$ - Samples of the form $(s,a,r,s')$ 
                \item $k$ - Number of basis functions
                \item $\phi$ - Basis functions
                \item $\gamma$ - Discount factor
                \item $w$ - Current policy
            \end{itemize}
        \STATE $B \leftarrow \frac{1}{\delta}I$ \COMMENT{$(k \times k)$ matrix}
        \STATE $b \leftarrow 0$ \COMMENT{$(k \times 1)$ vector}
        \FORALL{$(s,a,r,s') \in D$}
            \STATE $B \leftarrow B - \frac{B\phi(s,a)\big(\phi(s,a) - \gamma\phi(s',\pi(s'))\big)^TB}{1 + \big(\phi(s,a) - \gamma\phi(s',\pi(s'))\big)^TB\phi(s,a)}$
            \STATE $b \leftarrow b + \phi(s,a)r$
        \ENDFOR
        \STATE $\tilde{w} \leftarrow Bb$
        \RETURN $\tilde{w}$
    \end{algorithmic}
    }
\end{algorithm}

\subsection{Massively Parallel Least-Squares Policy Iteration}

We chose to focus on LSPI because LSTD$Q$ does not require any algorithmic changes in order to support a massively parallel implementation. This is because LSPI is expressed in terms of vector and matrix operations, which are parallelizable. However, this still requires implementation changes and leaves room for environment specific optimizations.

Knowing that we wanted to demonstrate the capabilities of a GPU for massively parallel acceleration, the first decision was what API to use. We chose CUDA because our test machine uses an nVidia GPU and several colleagues have experience with CUDA programming.

The next step was translating LSPI into standardized linear algebra operations, using the basic linear algebra subprograms (BLAS) library specification \cite{blas}. The BLAS specification was created to facilitate high performance computing for linear algebra operations: developers can write software which uses the BLAS specification and users can compile a BLAS implementation which is optimized for their hardware. The CUDA software development kit includes a library called cuBLAS which provides a BLAS interface for use on the GPU. Algorithm \ref{lstdq:blas} contains the pseudocode for LSTD$Q$ converted into BLAS operations.

\begin{algorithm}
\caption{LSTD$Q$ BLAS Implementation}
\label{lstdq:blas}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \REQUIRE
            \begin{itemize} 
                \item All input should be explicitly passed into GPU memory
                \item $D$ - Samples of the form $(s,a,r,s')$ 
                \item $k$ - Number of basis functions
                \item $\phi$ - Basis functions
                \item $\gamma$ - Discount factor
                \item $w$ - Current policy
            \end{itemize}
        \STATE $B \leftarrow scal(I, \delta)$ \COMMENT{$(k \times k)$ matrix}
        \STATE $b \leftarrow 0$ \COMMENT{$(k \times 1)$ vector}
        \FORALL{$(s,a,r,s') \in D$}
            \STATE $x \leftarrow scal(\phi(s',\pi(s')), \gamma)$
            \STATE $x \leftarrow axpy(\phi, x, -1)$
            \STATE $x \leftarrow scal(x, -1)$ \COMMENT{Now $x = \phi(s,a) - \gamma\phi(s',\pi(s'))$}
            \STATE $y \leftarrow gemv(B, \phi(s,a))$
            \STATE $z \leftarrow gemv(x, B)$
            \STATE $num \leftarrow ger(y, z)$ \COMMENT{Calculate the numerator, see Equation \eqref{eq:lspi}}
            \STATE $denom \leftarrow 1 + dot(\phi(s,a), z)$ \COMMENT{Calculate the denominator, see Equation \eqref{eq:lspi}}
            \STATE $\Delta B \leftarrow scal(num, \frac{1}{denom})$
            \STATE $B \leftarrow axpy(\Delta B, B, -1)$
            \STATE $\Delta b \leftarrow scal(\phi(s,a), r)$
            \STATE $b \leftarrow axpy(\Delta b, b)$
        \ENDFOR
        \STATE $\tilde{w} \leftarrow gemv(B, b)$
        \RETURN $\tilde{w}$
    \end{algorithmic}
    }
\end{algorithm}

The functions utilized for the BLAS implementation are as follows:

{\fontsize{12}{5}\selectfont
\begin{itemize}
    \item $scal$ - Performs vector-scalar multiplication
    \item $axpy$ - Adds two vectors together with a scalar multiplier
    \item $gemv$ - Performs vector-matrix multiplication
    \item $ger$ - Performs vector-vector multiplication
    \item $dot$ - Calculates the dot product of two vectors
\end{itemize} 
}

Since all of the scalar values used are within BLAS function calls, no explicit management is necessary to pass this data to the GPU. However, matrix and vector values must have memory on the GPU explicitly allocated and copied from device memory. In our code we solved this problem by using the $thrust$ library, which handles this automatically \cite{thrust}.

There are a few key portions of this algorithm which could benefit from custom functions to execute on the GPU. The first is the initialization of $B$ which involves generating the identity matrix by iterating over each element and determining the value. This could be optimized by writing a function which unrolls the loop and checks each element at the same time. The second is the computation of $\phi(s',\pi(s'))$ and $\phi(s,a)$ which may involve drawing data from the samples in a way that is not executed on the GPU. The third is the computation $\pi(s')$, which may compute the maximum of the values using the CPU and also relies on the evaluation of the basis function for multiple state-action pairs.

We could write custom CUDA kernels to perform these operations on the GPU in a custom manner; however, using just the BLAS functions provides an easy way to switch between CPU and GPU libraries for comparison. In addition, any optimizations we perform would not impact the performance scaling, which is the key result we expect to improve with GPU acceleration. More discussion of optimization and its potential impact on our results can be found in Chapter \ref{chap:evaluation}.

\subsection{Semi-Online Least-Squares Policy Iteration}

Offline learning techniques are typically used to process samples from an environment before an agent interacts with that environment. The limitation of these algorithms is that they cannot alter an agent's behavior while the agent is interacting in the environment. An online learning technique will usually be used if an agent's behavior needs to adapt dynamically to the environment. However, in a performance constrained environment it may not be feasible to perform the update step of an online reinforcement learning algorithm at every action selection.

We propose an algorithm which uses LSPI to process samples collected by an online agent and periodically updates the agent's policy. This approach allows for an agent's policy to be updated dynamically in an environment while eliminating the need to always perform an update step after selecting an action. The offline update process can be throttled to ensure the agent meets strict timing requirements for action selection, with the only drawback being increased delays between policy updates.

To demonstrate this approach with LSPI, we present Algorithm \ref{alg:olspi}. The algorithm works by collecting samples from the agent in the environment and executing LSPI once a pre-defined number of samples has been collected. Each time this number of new samples is collected the entire batch of samples is passed in. The number of samples to collect before each iteration should be carefully controlled to ensure that even the first update step provides a noticeable benefit to the agent.

\begin{algorithm}
\caption{Semi-Online LSPI}
\label{alg:olspi}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \REQUIRE
            \begin{itemize} 
                \item $n$ - Number of samples per batch
                \item $N$ - Maximum number of samples to collect
                \item $k$ - Number of basis functions
                \item $\phi$ - Basis functions
                \item $\gamma$ - Discount factor
                \item $\epsilon$ - Stopping criterion
                \item $\alpha$ - Exploration rate
                \item $w_0$ - Initial policy
            \end{itemize}
        \STATE $w \leftarrow w_0$ \COMMENT{$w$ represents the current policy weights}
        \STATE $D \leftarrow 0$
        \STATE $t \leftarrow 0$
        \WHILE{$t < N/n$}
            \FOR{$i = 0$ to $n$}
                \STATE Observe the state $s$
                \STATE Select action $a$, with exploration rate $\alpha$
                \STATE Observe the reward $r$
                \STATE Observe the final state $s'$
                \STATE Append $(s,a,r,s')$ to $D$
            \ENDFOR
            
            \STATE CreateThread($w \leftarrow LSPI(D, k, \phi, \gamma, \epsilon, w)$)
        \ENDWHILE
    \end{algorithmic}
    }
\end{algorithm}

In Algorithm \ref{alg:olspi} sample collection continues while the last batch of samples is processed. It would also be reasonable to pause sample collection until the policy update is completed. As long as the exploration rate remains in effect LSPI will continue to converge on the optimal policy. Additionally, the value of $N$ can be infinite if an agent that never ceases to update its policy is desired. For agents that do cease to update their policy it would be beneficial to stop exploring once the policy is finalized. In fact, agents which do not collect samples while the policy is being updated could also cease exploration during that period, resulting in an agent which switches between a policy mode and an exploration mode in order to maximize reward while maintaining exploration for successive policy iterations.

\section{Integration with Quake III}

The source code for Quake III is available on github under an open source license \cite{q3code}. To perform the experiments we implemented LSPI using CUDA acceleration and integrated this code into the Quake III source code.

The Quake III solution is separated into multiple projects representing different parts of the game engine and implementation; for our work we only modified the \emph{game}, \emph{botlib}, and \emph{server} projects, with most of the work occurring in the \emph{game} project. Modifications to \emph{botlib} were required to allow our bots to peek into the state of the environment without affecting it, which caused bugs when our bot checked the state of the environment more frequently than the native bots. The modifications to \emph{server} were only to facilitate the changes in \emph{botlib}.

Recall from Section \ref{chap:games:quake} that the Quake III AI is separated into four levels which are further broken into subsections. The \emph{botlib} provides the implementation for most of the lower two levels. The remaining logic is contained in the \emph{game} project, including the AI Network which guides actions at a high level. Our bot was implemented largely by supplanting the AI Network's finite state machine and allowing more frequent and flexible state transitions.

\subsection{Understanding the AI Optimizations}

Generic AI systems generally function on the assumption that we alternate between a few states: environment observation, action selection, action execution, and reward collection. With this flow it is very easy to change AI implementations by simply taking the environment observations and replacing the action selection step. However, in a performance constrained environment it may not be possible to follow this flow.

Suppose we had an agent acting in the Quake III environment called Cindy. Cindy's first action (as with every Quake III bot) is to search for a long-term goal. This can be an item or an area on the map. Because Cindy already has some knowledge about the arena, she decides she wants to go to the other side of the map and pick up the rocket launcher. However, before she moves forward she looks around at the nearby items. Since there are only other weapons and no armor, and Cindy's preferences are heavily weighted toward the rocket launcher, she decides to just head toward her current long-term goal.

After Cindy takes a step the world moves forward a step as other agents in the environment take similar actions. Now Cindy has the choice to re-evaluate her decision, but should she? After all, she just took a full profile of her environment and selected her choices, with little to no environmental changes occurring. It is very likely that Cindy would just make the same decision again and it takes much less thought to re-evaluate the environment.

Once again, Cindy takes a step and the world move forward. This time, however, an enemy has become visible. Even though Cindy only decided on her current task two steps ago, she decides to change her goal to killing the enemy. Notice that Cindy did not reconsider her entire environment: she could have realized that the starting weapon was not a good choice to attack her enemy with, resulting in turning around to grab a nearby weapon. Instead, Cindy only evaluates some small portion of her environment at each step and unless something critical in that portion changes, such as an enemy appearing, she will continue assuming the last decision she made was valid.

Cindy's behavior essentially represents the logic that the native Quake III bots follow. This performance optimization was critical for allowing several bots to fight in a match when desktop processors were still under the 1GHz limit and universally constrained to a single core. Unfortunately, this highly tangled system makes integrating a new AI structure very difficult. The next section discusses how we modified the AI structure to support a more traditional model for our own bots while still sharing code with the native bots.

\subsection{Using the Finite State Machine}

The existing finite state machine code is separated into two types of methods: transition methods and node execution methods. The transition methods generally perform some minor tasks like print out chat responses or tracking the number of node transition the bot executes, and then it sets a \emph{AI node} function pointer to the node execution method for that specific transition. From Cindy's example, when the enemy appeared this would trigger a state transition by calling the \emph{battle fight} node transition function, called \emph{enter battle fight}. Before calling this function the AI node would be referencing the node \emph{seek long-term goal}, but after the transition function this would reference the node \emph{battle fight}.

Rather than limiting the bots to executing only a single node in any given game step, each node execution function returns \emph{true} when it has finished executing and \emph{false} if it has called a transition method to a node before it reached the end of the current node. This causes the code calling the nodes to continue calling until a node reaches completion.

\begin{algorithm}
\caption{Node Execution Method}
\label{ainode}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \STATE Check for Special Nodes: \emph{Observer}, \emph{Intermission}, \emph{Dead}
        \STATE Ensure Movement Mode: \emph{swimming}, \emph{rocket jump}, \emph{etc}
        \STATE Execute Map Scripts
        \STATE Execute Node Scripts
        \STATE Setup Movement
        \RETURN \emph{true}
    \end{algorithmic}
    }
\end{algorithm}

Each execution node is formatted in the same way, following the pattern described in Algorithm \ref{ainode}. Intermixed among these primary steps are key conditionals which check portions of the state to see if a transition is necessary. If a transition is deemed necessary prior to step $5$, the node will return \emph{false}.

In order to use as much of the node execution method as possible, our bot needed to do two things: skip any conditionals which could change state and execute any possible state changes when the first node is called each step. There are some exceptions to this rule required to maintain proper bot behavior. For example, there is a node called \emph{seek activate entity} which is used to open doors by shooting buttons. We did not want to modify this code as it could fundamentally break the bot's ability to navigate the map.

\begin{algorithm}
\caption{Modified Node Execution Method}
\label{ainode:custom}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \STATE Check for Special Nodes: \emph{Observer}, \emph{Intermission}, \emph{Dead}
        \STATE Ensure Movement Mode: \emph{swimming}, \emph{rocket jump}, \emph{etc}
        \IF{Custom Bot and No Action Chosen}
            \STATE Observe Environment
            \STATE Get Action
            \IF{Bot Wants to Switch Nodes}
                \STATE Call Transition Method
                \RETURN \emph{false}
            \ENDIF
        \ENDIF
        \STATE Execute Map Scripts
        \STATE Execute Node Scripts
        \STATE Setup Movement
        \RETURN \emph{true}
    \end{algorithmic}
    }
\end{algorithm}

To achieve these goals we added a new flag called \emph{bottype} to the bot's personal state data. By default this value is $0$, so we used the values $1$ to represent LSPI agents and $2$ to represent gradient policy agents. Once we knew if a given bot was native or our own, we simply modified all of the nodes to ignore state transitions if it was our bot, and we added a new conditional demonstrated in steps $3-10$ of Algorithm \ref{ainode:custom}.

\subsection{Actions and Goals}

As our code was using the AI Network's finite state machine, our code was also limited by the actions available in this state machine. The finite state machine contains twelve nodes total; however, half of the nodes are maintained for special scenarios including bot death and match intermissions. As a result there are only six nodes available, each of which maps to an action available to our agent. The resulting actions are as follows:

\begin{itemize}
    \item \emph{Seek Long-term Goal} - Selects an item or location across the entire map and attempts to reach that goal. After an extended period of time has passed the bot will re-evaluate its goal.
    \item \emph{Seek Nearby Goal} - Assumes a goal has already been selected and moves toward the goal. After a short period of time the bot will re-evaluate its goal.
    \item \emph{Battle Fight} - The enemy becomes the goal and the bot will attempt to do combat maneuvers while attacking the enemy.
    \item \emph{Battle Chase} - The bot will move toward the last known location of the enemy in an effort to catch and kill them.
    \item \emph{Battle Retreat} - The bot selects a goal far away and moves toward it in an attempt to flee from the battle.
    \item \emph{Battle Nearby Goal} - The bot selects a nearby item to improve its odds of defeating the enemy.
\end{itemize}
Note that all of the actions prefixed \emph{Battle} are only valid if there is an enemy, and any actions without this prefix are only valid if there is no enemy.

As far as goal selection and seeking is concerned, our bots do not entirely follow the same rules as the native bots. In the default Quake III source code, attempting to find a nearby goal automatically updates the bot's goal state. In addition to this, selecting a goal creates sets a value referred to as an \emph{avoid timeout}. This value specifies a period of time during which the bot should ignore that goal, which means that any further searches before the timeout expires will skip that goal. 

Since our bot searches for goals every step, we had to modify the code to pass in a parameter specifying if we were trying to peek without changing the state. However, even this was not complete enough since the node \emph{seek nearby goal} does not actually search for nearby goals. As a result we had to modify the code to pick a nearby goal whenever attempting to transfer into that state. In addition, since the bots do not actually detect whether an item is present and we do not share the avoid scheme of the native bots, we modified the bots to add an avoid timeout to any item which they were potentially touching. The end result should be equivalent to the native bots; however, we did notice a few minor bugs where the bot ran out of available goals. Luckily this happens infrequently, is handled gracefully by the game (by resetting avoid timers), and is avoided by good policies.

Goal selection is a limiting factor for our bots: although we can direct the bot to look for a goal through the seeking and nearby goal states, we have no control over the goal selection itself. As a result, we rely on the Quake III AI implementation to decide what items our bot should pick up and whether the bot wants to roam or defend a key location. This means that some of our results do not properly demonstrate the full capabilities of a machine learning AI in this environment; however, we were still able to create bots capable of repeatedly defeating the native bots.