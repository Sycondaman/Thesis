\chapter{Massively Parallel Learning Algorithms}
\label{chap:implementation}

As discussed in the background section, offline learning algorithms have the potential to learn behavior for agents acting in performance constrained environments. To demonstrate this potential we chose to use Least-Squares Policy Iteration (LSPI) and compare it to an online policy gradient algorithm. This chapter presents these algorithms in mathematical detail and suggests a modification to LSPI to support its use as an online algorithm for performance constrained environments. Some of the derivation steps of the core algorithms have been left out, for more information please see the referenced sources. Details on how we integrated these algorithms in the Quake III environment can be found in Chapter \ref{chap:evaluation}. 

\section{Least-Squares Policy Iteration}

Least-Squares Policy Iteration (LSPI) is an offline, model-free reinforcement learning algorithm proposed by Michail Lagoudakis and Ronald Parr \cite{lspi}. LSPI takes as input a set of samples from the environment and performs approximate policy iteration using a function called LTSD$Q$ to compute the state-action value function during the policy evaluation step. LSTD$Q$ stands for Least-Squares Temporal-Difference Learning for the State-Action Value Function; in other words, it is a least-squares temporal difference algorithm for learning $\hat{Q}^\pi$, the least-squares fixed-point approximation to the sate-action value function, $Q^\pi$.

LSPI uses a linear architecture for $\hat{Q}^\pi$, which means the values are approximated using a linear combination of $k$ basis functions, commonly referred to as features:

\[
    \hat{Q}^\pi(s,a;w) = \sum_{j=1}^k \phi_j(s,a)w_j
\]

In this formula, $\phi(s,a)$ is a fixed function of $s$ and $a$ and $w_j$ represents the weight for $\phi_j$. We can represent $\phi(s,a)$ as a column vector with an entry for each basis function:


\[
    \phi(s,a) = \begin{pmatrix}
       \phi_1(s,a) \\ \phi_2(s,a) \\ \vdots \\ \phi_k(s,a)
    \end{pmatrix}
\]

$\hat{Q}^\pi$  can now be expressed compactly:

\[
    \hat{Q}^\pi = \Phi w^\pi
\]
where $w^\pi$ is a column vector with length k and $\Phi$ is a matrix of the form

\[
    \Phi = \begin{pmatrix}
        \phi(s_1,a_1,)^T \\ \vdots \\ \phi(s,a)^T \\ \vdots \\ \phi(s_{|\mathcal{S}|},a_{|\mathcal{A}|})^T
    \end{pmatrix}
\]
Each row of $\Phi$ represents the basis function values for a specific state-action pair. The basis functions are required to be linearly independent in LSPI which also means the columns of $\Phi$ are linearly independent.

To find $\hat{Q}^\pi$, we force it to be a fixed point under the Bellman operator:

\[
    T_\pi\hat{Q}^\pi \approx \hat{Q}^\pi
\]
To ensure this is true we must project $T_\pi\hat{Q}^\pi$ to a point in the space spanned by the basis functions:

\[
    \hat{Q}^\pi = \Phi(\Phi^T\Phi)^{-1}\Phi^T(\mathcal{R} + \gamma P\Pi_\pi\hat{Q}^\pi)
\]
From this formula we can derive the desired solution:
\[
    \Phi^T(\Phi - \gamma P\Pi_\pi\Phi)w^\pi = \Phi^T\mathcal{R}
\]

To achieve the final project, we define $\mu$ as a probability distribution over $(s,a)$ and $\Delta_\mu$ as a diagonal matrix with the projection weights $\mu(s,a)$. This results in the weighted least-squares fixed-point solution:

\[
    w^\pi = (\Phi^T\Delta_\mu(\Phi - \gamma P\Pi_\pi\Phi))^{-1}\Phi^T\Delta_\mu\mathcal{R}
\]

Once again assuming there are $k$ linearly independent basis functions, learning $\hat{Q}^\pi$ is equivalent to learning the weights $w^\pi$ of $\hat{Q}^\pi = \Phi w^\pi$. The exact values can be computed by solving the linear system

\[
    Aw^\pi = b
\]
where

\[
    \begin{array}{rcl}
        A &=& \Phi^T\Delta_\mu(\Phi - \gamma P\Pi_\pi\Phi) \\
        b &=& \Phi^T\Delta_\mu\mathcal{R}
    \end{array}
\]
and $\mu$ is a probability distribution over the state-action space which defines the weights of the projection. $A$ and $b$ can be learned using samples and the learned linear system can be solved for $\tilde{w}^\pi$. 

The equations for $A$ and $b$ can be reduced until it is clear that $A$ is the sum of many rank one matrices of the form:

\[
    \phi(s,a)(\phi(s,a) - \gamma\phi(s',\pi(s')))^T
\]
and $b$ is the sum of many vectors of the form:

\[
    \phi(s,a)R(s,a,s')
\]
These summations are then taken over $s$, $a$, and $s'$ and weighted according to $\mu(s,a)$ and $\mathcal{P}(s,a,s')$. It is generally impractical to compute this summation over all values of $(s,a,s')$; however, samples taken from the environment can be used to obtain arbitrarily close approximations.

Given a finite set of samples of the form

\[
    D = { (s_i,a_i,r_i,s_i') | i = 1,2,...,L}
\]
we can learn $A$ and $b$:

\[
    \begin{array}{rcl}
        \tilde{A} &=& \frac{1}{L}\displaystyle\sum_{i=1}^L[\phi(s_i,a_i)(\phi(s_i,a_i) - \gamma\phi(s_i',\pi(s_i')))^T] \\
        \tilde{b} &=& \frac{1}{L}\displaystyle\sum_{i=1}^L[\phi(s_i,a_i)r_i)]
    \end{array}
\]
In practical computations $L$ is finite, allowing us to drop the factor $(1/L)$ since it cancels out. Because samples contribute additively an incremental update rule can be constructed:

\[
    \begin{array}{rcl}
        \tilde{A}^{(t+1)} &=& \tilde{A}^{(t)} + \phi(s_t,a_t)(\phi(s_t,a_t) - \gamma\phi(s_t',\pi(s_t')))^T \\
        \tilde{b}^{(t+1)} &=& \tilde{b}^{(t)} + \phi(s_t,a_t)r_t
    \end{array}
\]

Using these formulas, an algorithm can be constrcuted to learn the weighted least-squares fixed-point approximation of $\hat{Q}^\pi$: LSTD$Q$. The name of this algorithm is derived due to similarities with LSTD, Least-Squares Temporal Difference Learning. With LSTD$Q$ the same set of samples $D$ can be used to learn $\hat{Q}^\pi$ of any policy as long as $\pi(s')$ exists for each $s' \in D$. Another nice feature of LSTD$Q$ is the lack of any restriction on how samples are collected. This means we can learn a policy from pre-existing samples, combine samples from multiple agents, or even take samples from user input.

In terms of performance, LSTD$Q$ requires $O(k^2)$ independent of the size of the state and action spaces. Each update to the matrices $\hat{A}$ and $\hat{b}$ has a cost of $O(k^2)$ and a one-time cost of $O(k^3)$ is required to solve the system. An optimized version of LSTD$Q$ is also possible using the Sherman-Morrison formula for a recursive least-squares computation of the inverse. Let $B^{(t)}$ be the inverse of $\hat{A}^{(t)}$:

    \begin{equation}
        \label{eq:lspi}
    B^{(t)} = B^{(t-1)} - \frac{B^{(t-1)}\phi(s_t,a_t)\big(\phi(s_t,a_t) - \gamma\phi(s_t',\pi(s_t'))\big)^TB^{(t-1)}}{1 + \big(\phi(s_t,a_t) - \gamma\phi(s_t',\pi(s_t'))\big)^TB^{(t-1)}\phi(s_t,a_t)}
    \end{equation}
The pseudocode for LSTD$Q$ is shown in Algorithm \ref{lstdq} and the pseudocode for LSPI is shown in \ref{lspi}. \cite{lspi}

\begin{algorithm}
\caption{LSPI}
\label{lspi}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \REQUIRE
            \begin{itemize} 
                \item $D$ - Samples of the form $(s,a,r,s')$ 
                \item $k$ - Number of basis functions
                \item $\phi$ - Basis functions
                \item $\gamma$ - Discount factor
                \item $\epsilon$ - Stopping criterion
                \item $w_0$ - Initial policy
            \end{itemize}
        \STATE $w' \leftarrow w_0$
        \REPEAT
            \STATE $w \leftarrow w'$
            \STATE $w' \leftarrow$ LSTD$Q (D, k, \phi, \gamma, w)$
        \UNTIL{$(\parallel w - w' \parallel < \epsilon)$}
        \RETURN $w$
    \end{algorithmic}
    }
\end{algorithm}

\begin{algorithm}
\caption{LSTD$Q$}
\label{lstdq}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \REQUIRE
            \begin{itemize} 
                \item $D$ - Samples of the form $(s,a,r,s')$ 
                \item $k$ - Number of basis functions
                \item $\phi$ - Basis functions
                \item $\gamma$ - Discount factor
                \item $w$ - Current policy
            \end{itemize}
        \STATE $B \leftarrow \frac{1}{\delta}I$ \COMMENT{$(k \times k)$ matrix}
        \STATE $b \leftarrow 0$ \COMMENT{$(k \times 1)$ vector}
        \FORALL{$(s,a,r,s') \in D$}
            \STATE $B \leftarrow B - \frac{B\phi(s,a)\big(\phi(s,a) - \gamma\phi(s',\pi(s'))\big)^TB}{1 + \big(\phi(s,a) - \gamma\phi(s',\pi(s'))\big)^TB\phi(s,a)}$
            \STATE $b \leftarrow b + \phi(s,a)r$
        \ENDFOR
        \STATE $\tilde{w} \leftarrow Bb$
        \RETURN $\tilde{w}$
    \end{algorithmic}
    }
\end{algorithm}

\subsection{Massively Parallel Least-Squares Policy Iteration}

It turns out that LSPI does not require any algorithmic changes in order to support a massively parallel implementation. This is because LSPI is expressed in terms of vector and matrix operations, which are already inherently parallel. However, this still requires implementation changes and leaves room for environment specific optimizations.

Knowing that we wanted to demonstrate the capabilities of a GPU for massively parallel acceleration, the first decision was what software stack to use. We chose CUDA because our test machine uses an nVidia GPU and several colleagues have experience with CUDA programming.

The next step was translating LSPI into standardized linear algebra operations, using the basic linear albegra subprograms (BLAS) library specification. The BLAS specification was created to facilitate high performance computing for linear algebra operations: developers can write software which uses the BLAS specification and users can compile a BLAS implementation which is optimized for their hardware. The CUDA software development kit includes a library called cuBLAS which provides a BLAS interface for use on the GPU. Algorithm \ref{lstdq:blas} contains the pseudocode for LSTD$Q$ converted into BLAS operations.

\begin{algorithm}
\caption{LSTD$Q$ BLAS Implementation}
\label{lstdq:blas}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \REQUIRE
            \begin{itemize} 
                \item $D$ - Samples of the form $(s,a,r,s')$ 
                \item $k$ - Number of basis functions
                \item $\phi$ - Basis functions
                \item $\gamma$ - Discount factor
                \item $w$ - Current policy
            \end{itemize}
        \STATE $B \leftarrow scal(I, \delta)$ \COMMENT{$(k \times k)$ matrix}
        \STATE $b \leftarrow 0$ \COMMENT{$(k \times 1)$ vector}
        \FORALL{$(s,a,r,s') \in D$}
            \STATE $x \leftarrow scal(\phi(s',a'), \gamma)$
            \STATE $x \leftarrow axpy(\phi, x, -1)$
            \STATE $x \leftarrow scal(x, -1)$ \COMMENT{Now $x = \phi(s,a) - \gamma\phi(s',\pi(s'))$}
            \STATE $y \leftarrow gemv(B, \phi(s,a))$
            \STATE $z \leftarrow gemv(x, B)$
            \STATE $num \leftarrow ger(y, z)$ \COMMENT{Calculate the numerator, see \eqref{eq:lspi}}
            \STATE $denom \leftarrow 1 + dot(\phi(s,a), z)$ \COMMENT{Calculate the denominator, see \eqref{eq:lspi}}
            \STATE $\Delta B \leftarrow scal(num, \frac{1}{denom})$
            \STATE $B \leftarrow axpy(\Delta B, B, -1)$
            \STATE $\Delta b \leftarrow scal(\phi(s,a), r)$
            \STATE $b \leftarrow axpy(\Delta b, b)$
        \ENDFOR
        \STATE $\tilde{w} \leftarrow gemv(B, b)$
        \RETURN $\tilde{w}$
    \end{algorithmic}
    }
\end{algorithm}

The functions utilized for the BLAS implementation are as follows:

{\fontsize{12}{5}\selectfont
\begin{itemize}
    \item $scal$ - Performs vector-scalar multiplication
    \item $axpy$ - Adds two vectors together with a scalar multiplier
    \item $gemv$ - Performs vector-matrix multiplication
    \item $ger$ - Performs vector-vector multiplication
    \item $dot$ - Calculates the dot product of two vectors
\end{itemize} 
}

You'll notice that some of the operations we utilized pass in matrices in place of vectors. This is possible because a matrix can be represented by a vector and operations such as a addition and scalar multiplication are identical regardless of whether a vector represents a matrix.

Although more optimizations can be made to take advantage of massively parallel computing, using BLAS functions provides an easy way to switch between CPU and GPU libraries for comparison and the majority of remaining optimizations would all be hardware specific. For this reason we chose not to implement performance optimizations for either the CPU or GPU implementations. More discussion of this and the potential impact of our work can be found in the experiments chapter.

\subsection{Online Least-Squares Policy Iteration}



\section{Policy Gradient}

The term `Policy Gradient' describes a class of machine learning techniques which apply gradient ascent optimization to the policy value function. Given the policy value function, $p(\theta)$, policy gradient algorithms can be succinctly described by the following formula:

\[
    \theta_{n+1} = \theta_{n} + \alpha\nabla_\theta p(\theta_{n})
\]
In this formula $\theta$ represents the policy parameters and $\alpha$ is referred to as the step size or learning rate. \cite{norvig}\cite{bishop} Despite the apparent simplicity of this formula, calculating the value of $\nabla_\theta p(\theta)$ is not always straightforward. In this section we provide the mathematical background for the particular online gradient policy algorithm we used for comparison purposes.

To calculate $\nabla_\theta p(\theta)$, we must first ensure that $p(\theta)$ is differentiable with respect to $\theta$. Assume we have a basis function, $J_\theta(s,a)$ which is used to generate a policy as follows:

\[
    \pi_\theta(s,a) = \argmax_a J_\theta(s,a)
\]
The use of the $argmax$ function may result in a discontinous distribution. This can be fixed very simply by using a probabilistic policy \cite{olpomdp:lecture}:

\[
    \pi_\theta(s,a) = Pr(a|s) = \frac{e^{J_\theta(s,a)/T}}{\sum\limits_{a'\in A} e^{J_\theta(s,a')/T}}
\]

Using this model we can then derive a value for $\nabla_\theta p(\theta)$:
\[
    \begin{aligned}
        \nabla_\theta p(\theta) &= \sum_a (\nabla_\theta\pi_\theta(s_0,a))R(a) \\
        &= \sum_a \pi_\theta(s_0, a) \nabla_\theta log(\pi_\theta(s_0, a))R(a) \\
    \end{aligned}
\]
After simplifying the gradient calculation we arrive at the following:
\[
    \begin{aligned}
        \nabla_\theta p(\theta) &= \sum_a\pi_\theta(s_0,a)g_\theta(s_0,a)R(a) \\
        g_\theta(s_0,a) &= f_\theta(s_0, a) - \sum\limits_{a'} \pi_\theta(s_0,a')f_\theta(s_0,a')
    \end{aligned}
\]

In order to estimate the gradient from samples we use basic Monte Carlo techniques: execute the policy while making small changes to $\theta$. The result is an online algorithm called OLPOMDP, proposed by Baxter, Bartlett, and Weaver. \cite{olpomdp} Since the policy involved is probablistic, exploration of the environment happens naturally when selecting actions. With each action we then update the eligibility trace, $e$, by estimating the value of the gradient for that action. The eligibility trace is then used to update $\theta$ according to the reward given for each sample in the environment. The value $\beta$ is referred to as the bias and controls the weight of previous experiences in the environment. Increasing $\beta$ biases the algorithm toward past experiences while also increasing the variance. The pseudocode for this algorithm is given in Algorithm \ref{olpomdp}.

\begin{algorithm}
\caption{OLPOMDP}
\label{olpomdp}
    {\fontsize{12}{10}\selectfont
    \begin{algorithmic}[1]
        \REQUIRE
            \begin{itemize} 
                \item $\beta \in [0,1)$ 
                \item $T > 0$ 
                \item $\theta_0 \in \mathbb{R}^K$ 
            \end{itemize}
    \STATE Set $e = 0$
    \FOR{$t = 0$ to $\infty$}
        \STATE Observe the current state, $s$
        \STATE Select action $a$ according to $\pi_\theta(s)$
        \STATE Execute $a$
        \STATE Observe reward $r$
        \STATE $e \leftarrow \beta e + f(s,a) - \sum\limits_{a'}\pi_\theta(s,a')f_\theta(s,a')$
        \STATE $\theta \leftarrow + \alpha \cdot r \cdot e$
    \ENDFOR
    \end{algorithmic}
    }
\end{algorithm}

