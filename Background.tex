\section{Background}

\subsection{Markov Decision Process}

A \emph{Markov Decision Process} (MDP) is a specification of a sequential decision process in a fully observable environment with three components: the initial state, the transition model, and the reward function. An MDP can be represented by the 5-tuple $(\mathcal{S, A, P}, R, \gamma)$ where: $\mathcal{S} = \{s_1, s_2, ..., s_n\}$ is a finite set of states; $\mathcal{A} = \{a_1, a_2, ..., a_m\}$ is a finite set of actions; $\mathcal{P}$ is a Markovian transition model and $\mathcal{P}(s, a, s')$ is the probability of making a transition to state $s'$ when taking action $a$ while in state $s$; $R$ is a reward or cost function such that $R(s, a, s')$ is the reward for the transition represented by $(s, a, s')$; and $\gamma \in [0, 1)$ is the discount factor for future rewards. Using this definition we can describe the expected reward for a given state-action pair $(s, a)$:

\[
		\mathcal{R}(s, a) = \sum_{s' \in \mathcal{S}} \mathcal{P}(s, a, s')R(s, a, s')
\]

The solution to an MDP is defined as the policy, denoted by $\pi$. Given the stochastic nature of an MDP, a given policy is rated based on the expected utility of actions selected by the policy. The optimal policy for a given MDP is the policy which yields the highest expected utility and is denoted by $\pi^*$. An agent following policy $\pi^*$ acts by determining the current state $s$, and executing the action specified by $\pi^*(s)$.

When defining the expected utility of an MDP, the horizon must first be described as finite or infinite. A finite horizon occurs when there is a fixed time $N$ after which actions have no effect on reward or cost. An infinite horizon does not have any fixed limitation. This does not mean that state sequences are infinite, only that there is no fixed deadline. With an infinite horizon the optimal policy is stationary; that is, given state $s$ is the current state the optimal action is $\pi^*(s)$, regardless of the time. With a finite horizon the optimal policy changes over time and is described as nonstationary. For the remainder of this paper all MDPs will be assumed to have an infinite horizon.

Given an MDP with infinite horizon the utility of a state sequence can be defined as either additive or discounted. For additive rewards the utility of a state sequence is:

\[
		U_h([s_0,s_1,s_2,...]) = R(s_0) + R(s_1) + R(s_2) + ...
\]

For discounted rewards the utility of a state sequence is:

\[
		U_h([s_0,s_1,s_2,...]) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + ...
\]

The discount factor $\gamma$ is defined as a number between $0$ and $1$. From this it is clear that additive rewards is a special case of discounted rewards with $\gamma = 1$.  Note that as $\gamma$ approaches $0$, the utility function becomes weighted toward immediate rewards and as $\gamma$ approaches $1$ the utility function becomes weighted toward long term rewards. For the remainder of this paper all MDPs will be assumed to use discounted rewards with $\gamma \in [0, 1)$, precluding additive rewards.

\subsection{Policy Iteration}

Given a complete model of an MDP it is possible to use the policy iteration algorithm to determine an optimal policy. The policy iteration algorithm requires some initial policy $\pi_0$ followed by the repeated iteration of two steps: policy evaluation and policy improvement. Policy evaluation calculates the utility of policy $\pi_i$ such that $U_i = U^{\pi_i}$. Policy improvement computes a new policy based on $U_i$ in an attempt to improve on policy $\pi_i$. These two steps are repeated until the policy improvement step no longer yields a change in the utilities: $U_i \approx U_{i+1}$. 

To confirm that this results in an optimal policy, we must examine how the utility function is generated during each iteration. Assuming the goal is to maximize the utility, the optimal policy can be defined using the following function over the state space:

\[
		\pi^*(s) = \argmax_a \sum_{s'} T(s,a,s')U(s')
\]

The utility of a single state is defined by the Bellman equation:

\[
		U(s) = R(s) + \gamma \max_a \sum_{s'}T(s,a,s')U(s')
\]

This function is derived from the idea that the utility of each state is equivalent to the reward at that state plus the sum of the discounted rewards of all following states, assuming an optimal policy. For a given MDP, there are $n$ Bellman equations, $n$ states, and $n$ unknowns (the utilities of the states). Solving the Bellman equations provides an optimal policy; however, solving these non-linear equations is a very slow process. One possible solution is to use an iterative approaching using the Bellman update:

\[
		U_{i+1}(s) \gets R(S) + \gamma \max_a \sum_{s'}T(s,a,s')U_i(s')
\]

The utility function $U_i$ which results from policy iteration is a fixed point of the Bellman update, which means it is a solution to the Bellman equations and thus $\pi_i$ must be an optimal policy.

To perform the policy evaluation step we can use a simplified version of the Bellman equation:

\[
		U_i(s) = R(s) + \gamma \sum_{s'} T(s, \pi_i(s), s')U_i(s')
\]

By removing the max operator, the optimal policy can now be determined using linear algebra to solve the Bellman equations.

\subsection{Reinforcement Learning}

In practical control problems it is often the case that a complete model of the underlying MDP is not available. Most commonly, the transition model and reward function are unknown. To solve these problems it is beneficial to have agents which can learn optimal policies through the process of reinforcement learning. A typical reinforcement agent will generate a policy by acting within the environment and use positive and negative outcomes to reinforce good policy decisions and avoid bad policy decisions.

The observations of an agent acting an environment can be organized in tuples referred to as samples: $(s, a, r, s')$. This sample layout means the agent was in state $s$, took action $a$, received reward $r$, and ended up in state $s'$. These samples can be collected by interacting with the environment or by using a generative model of the environment. Some reinforcement learning algorithms also support policy generation by collecting samples from multiple agents or generative models.

\subsubsection{Online and Offline Reinforcement Learning}

\subsection{Least-Squares Policy Iteration}

\subsection{Policy Gradient}

\subsection{Artificial Intelligence in Video Games}

\subsubsection{Scripting}

\subsubsection{Dynamic Planning}

\subsubsection{Quake III Bots}

\subsection{CUDA Architecture}